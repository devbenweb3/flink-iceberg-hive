version: "3.5"

services:
  
  # --------------------------------------------------------------------------------------
  # Orchestration Layer 

  airflow-webserver:
    image: apache/airflow:latest
    container_name: airflow-webserver
    restart: always
    user: root 
    depends_on:
      - airflow-scheduler
      - airflow-db
    ports:
      - "8085:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        apt-get update && 
        apt-get install -y procps &&
        apt-get install -y --no-install-recommends openjdk-17-jre-headless &&
        export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 &&
        su - airflow -c "pip install apache-airflow-providers-apache-spark" &&
        airflow db init &&
        airflow webserver

  airflow-scheduler:
    image: apache/airflow:latest
    container_name: airflow-scheduler
    restart: always
    user: root 
    depends_on:
      - airflow-db
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
      - JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        apt-get update && 
        apt-get install -y procps &&
        apt-get install -y --no-install-recommends openjdk-17-jre-headless &&
        export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 &&
        su - airflow -c "pip install apache-airflow-providers-apache-spark" &&
        airflow scheduler

  airflow-db:
    image: postgres:13
    container_name: airflow-db
    restart: always
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5434:5432"
    volumes:
      - ./airflow-db-data:/var/lib/postgresql/data

  # --------------------------------------------------------------------------------------
  # BI Layer - Apache Superset

  superset_db:
    image: postgres:13
    container_name: superset_db
    restart: always
    environment:
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
      POSTGRES_DB: superset
    ports:
      - "5433:5432"
    volumes:
      - ./superset-db-data:/var/lib/postgresql/data

  superset:
    image: apache/superset:latest
    container_name: superset
    environment:
      - SUPERSET_CONFIG_PATH=/etc/superset/superset_config.py
    ports:
      - "8088:8088"
    volumes:
      - ./superset-config:/etc/superset
    depends_on:
      - trino
      - superset_db
    command: >
      /bin/sh -c "
      pip install psycopg2-binary &&
      pip install trino[sqlalchemy] &&
      superset db upgrade &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088
      "

  # --------------------------------------------------------------------------------------
  # Compute Engine

  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./spark_scripts:/opt/spark/scripts
      - ./spark_jars:/opt/spark/jars

  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master
    ports:
      - "8081:8081"
    volumes:
      - ./spark_scripts:/opt/spark/scripts
      - ./spark_jars:/opt/spark/jars

  trino:
    image: trinodb/trino:latest
    container_name: trino
    ports:
      - "8083:8080"
    volumes:
      - ./trino-config:/etc/trino
    depends_on:
      - hive-metastore

  # --------------------------------------------------------------------------------------
  # Storage Layer
  minio:
    image: minio/minio
    container_name: minio
    environment:
      - MINIO_ROOT_USER=admin
      - MINIO_ROOT_PASSWORD=password
      - MINIO_REGION_NAME=us-east-1
    ports:
      - 9001:9001
      - 9000:9000
    command: [ "server", "/data", "--console-address", ":9001" ]

  mc:
    depends_on:
      - minio
    image: minio/mc
    container_name: mc
    entrypoint: >
      /bin/sh -c "
      until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' && sleep 1; done;
      /usr/bin/mc rm -r --force minio/warehouse;
      /usr/bin/mc mb minio/warehouse;
      tail -f /dev/null
      " 

  metastore_db:
    image: postgres:11
    hostname: metastore_db
    container_name: metastore_db
    ports:
      - 5432:5432
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    command: [ "postgres", "-c", "wal_level=logical" ]
    healthcheck:
      test: [ "CMD", "psql", "-U", "hive", "-c", "SELECT 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - ./postgresscripts:/docker-entrypoint-initdb.d

  hive-metastore:
    hostname: hive-metastore
    container_name: hive-metastore
    image: 'starburstdata/hive:3.1.2-e.18'
    ports:
      - '9083:9083' # Metastore Thrift
    environment:
      HIVE_METASTORE_DRIVER: org.postgresql.Driver
      HIVE_METASTORE_JDBC_URL: jdbc:postgresql://metastore_db:5432/metastore
      HIVE_METASTORE_USER: hive
      HIVE_METASTORE_PASSWORD: hive
      HIVE_METASTORE_WAREHOUSE_DIR: s3://warehouse/
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: admin
      S3_SECRET_KEY: password
      S3_PATH_STYLE_ACCESS: "true"
      REGION: ""
      GOOGLE_CLOUD_KEY_FILE_PATH: ""
      AZURE_ADL_CLIENT_ID: ""
      AZURE_ADL_CREDENTIAL: ""
      AZURE_ADL_REFRESH_URL: ""
      AZURE_ABFS_STORAGE_ACCOUNT: ""
      AZURE_ABFS_ACCESS_KEY: ""
      AZURE_WASB_STORAGE_ACCOUNT: ""
      AZURE_ABFS_OAUTH: ""
      AZURE_ABFS_OAUTH_TOKEN_PROVIDER: ""
      AZURE_ABFS_OAUTH_CLIENT_ID: ""
      AZURE_ABFS_OAUTH_SECRET: ""
      AZURE_ABFS_OAUTH_ENDPOINT: ""
      AZURE_WASB_ACCESS_KEY: ""
      HIVE_METASTORE_USERS_IN_ADMIN_ROLE: "admin"
    depends_on:
      - metastore_db
    healthcheck:
      test: bash -c "exec 6<> /dev/tcp/localhost/9083"


networks:
  default:
    name: fih
